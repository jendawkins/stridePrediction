%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Multilayer Perceptron (MLP) Neural Network Function using MATLAB:       %
%  An implementation for Multilayer Perceptron Feed Forward Fully         %
%  Connected Neural Network with a sigmoid activation function. The       %
%  training is done using the Backpropagation algorithm with options for  %
%  Resilient Gradient Descent, Momentum Backpropagation, and Learning     %
%  Rate Decrease. The training stops when the Mean Square Error (MSE)     %
%  reaches zero or a predefined maximum number of epochs is reached.      %
%                                                                         %
%  Four example data for training and testing are included with the       %
%  project. They are generated by SharkTime Sharky Neural Network         %
%   (http://sharktime.com/us_SharkyNeuralNetwork.html)                    %
%                                                                         %
% Copyright (C) 9-2015 Hesham M. Eraqi. All rights reserved.              %
%                    hesham.eraqi@gmail.com                               %
%                                                                         %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Clear Variables, Close Current Figures, and Create Results Directory
function [NodesActivations, Weights, unipolarBipolarSelector] = MLP_NN(Samples,TargetClasses,trainProbs)

% mkdir('Results//'); %Directory for Storing Results

%% Configurations/Parameters
% dataFileName = 'sharky.spirals.points'; %sharky.linear.points - sharky.circle.points - sharky.wave.points - sharky.spirals.points
nbrOfNeuronsInEachHiddenLayer = [10 10]; %linear:[4] - circle:[10] - wave,spirals:[10 10]
nbrOfOutUnits = 100;
unipolarBipolarSelector = 0; %0 for Unipolar, -1 for Bipolar

learningRate = 0.15;
nbrOfEpochs_max = 50;

enable_resilient_gradient_descent = 1; %1 for enable, 0 for disable
learningRate_plus = 1.2;
learningRate_negative = 0.5;
deltas_start = 0.9;
deltas_min = 10^-6;
deltas_max = 50;

enable_decrease_learningRate = 0; %1 for enable decreasing, 0 for disable
learningRate_decreaseValue = 0.0001;
min_learningRate = 0.05;

enable_learningRate_momentum = 0; %1 for enable, 0 for disable
momentum_alpha = 0.05;

draw_each_nbrOfEpochs = 100;

%% Read Data
% importedData = importdata(dataFileName, '\t', 6);
% Samples = importedData.data(:, 1:length(importedData.data(1,:))-1);
% TargetClasses = importedData.data(:, length(importedData.data(1,:)));
% TargetClasses = TargetClasses - min(TargetClasses);
ActualClasses = -1*ones(size(TargetClasses));

%% Calculate Number of Input and Output NodesActivations
nbrOfInputNodes = length(Samples(1,:)); %=Dimention of Any Input Samples
% nbrOfOutUnits = ceil(log2(length(unique(TargetClasses)))) + !; %Ceil(Log2( Number of Classes ))

nbrOfLayers = 2 + length(nbrOfNeuronsInEachHiddenLayer);
nbrOfNodesPerLayer = [nbrOfInputNodes nbrOfNeuronsInEachHiddenLayer nbrOfOutUnits];

%% Adding the Bias as Nodes with a fixed Activation of 1
nbrOfNodesPerLayer(1:end-1) = nbrOfNodesPerLayer(1:end-1) + 1;
Samples = [ones(length(Samples(:,1)),1) Samples];

%% Calculate TargetOutputs %TODO needs to be general for any nbrOfOutUnits
TargetOutputs = zeros(length(TargetClasses), nbrOfOutUnits);
for i=1:length(TargetClasses)
    TargetOutputs(i,TargetClasses) = 1; 
%     if (TargetClasses(i) == 1)
%         TargetOutputs(i,:) = [1 unipolarBipolarSelector];
%     else
%         TargetOutputs(i,:) = [unipolarBipolarSelector 1];
%     end
end

%% Initialize Random Wieghts Matrices
Weights = cell(1, nbrOfLayers); %Weights connecting bias nodes with previous layer are useless, but to make code simpler and faster
Delta_Weights = cell(1, nbrOfLayers);
ResilientDeltas = Delta_Weights; % Needed in case that Resilient Gradient Descent is used
for i = 1:length(Weights)-1
    Weights{i} = 2*rand(nbrOfNodesPerLayer(i), nbrOfNodesPerLayer(i+1))-1; %RowIndex: From Node Number, ColumnIndex: To Node Number
    Weights{i}(:,1) = 0; %Bias nodes weights with previous layer (Redundant step)
    Delta_Weights{i} = zeros(nbrOfNodesPerLayer(i), nbrOfNodesPerLayer(i+1));
    ResilientDeltas{i} = deltas_start*ones(nbrOfNodesPerLayer(i), nbrOfNodesPerLayer(i+1));
end
Weights{end} = ones(nbrOfNodesPerLayer(end), 1); %Virtual Weights for Output Nodes
Old_Delta_Weights_for_Momentum = Delta_Weights;
Old_Delta_Weights_for_Resilient = Delta_Weights;

NodesActivations = cell(1, nbrOfLayers);
for i = 1:length(NodesActivations)
    NodesActivations{i} = zeros(1, nbrOfNodesPerLayer(i));
end
NodesBackPropagatedErrors = NodesActivations; %Needed for Backpropagation Training Backward Pass

zeroRMSReached = 0;
nbrOfEpochs_done = 0;

%% Iterating all the Data
MSE = -1 * ones(1,nbrOfEpochs_max);
for Epoch = 1:nbrOfEpochs_max
    
    for Sample = 1:length(Samples(:,1))
        %% Backpropagation Training
        %Forward Pass
        NodesActivations{1} = Samples(Sample,:);
        for Layer = 2:nbrOfLayers
            NodesActivations{Layer} = NodesActivations{Layer-1}*Weights{Layer-1};
            NodesActivations{Layer} = Activation_func(NodesActivations{Layer}, unipolarBipolarSelector);
            if (Layer ~= nbrOfLayers) %Because bias nodes don't have weights connected to previous layer
                NodesActivations{Layer}(1) = 1;
            else
                NodesActivations{Layer} = NodesActivations{Layer}.*trainProbs(Sample,:);
            end
        end
        
        % Backward Pass Errors Storage
        % (As gradient of the bias nodes are zeros, they won't contribute to previous layer errors nor delta_weights)
        NodesBackPropagatedErrors{nbrOfLayers} =  TargetOutputs(Sample,:)-NodesActivations{nbrOfLayers};
        for Layer = nbrOfLayers-1:-1:1
            gradient = Activation_func_drev(NodesActivations{Layer+1}, unipolarBipolarSelector);
            for node=1:length(NodesBackPropagatedErrors{Layer}) % For all the Nodes in current Layer
                NodesBackPropagatedErrors{Layer}(node) =  sum( NodesBackPropagatedErrors{Layer+1} .* gradient .* Weights{Layer}(node,:) );
            end
        end
        
        % Backward Pass Delta Weights Calculation (Before multiplying by learningRate)
        for Layer = nbrOfLayers:-1:2
            derivative = Activation_func_drev(NodesActivations{Layer}, unipolarBipolarSelector);    
            Delta_Weights{Layer-1} = Delta_Weights{Layer-1} + NodesActivations{Layer-1}' * (NodesBackPropagatedErrors{Layer} .* derivative);
        end
    end
    
    %% Apply resilient gradient descent or/and momentum to the delta_weights
    if (enable_resilient_gradient_descent) % Handle Resilient Gradient Descent
        if (mod(Epoch,200)==0) %Reset Deltas
            for Layer = 1:nbrOfLayers
                ResilientDeltas{Layer} = learningRate*Delta_Weights{Layer};
            end
        end
        for Layer = 1:nbrOfLayers-1
            mult = Old_Delta_Weights_for_Resilient{Layer} .* Delta_Weights{Layer};
            ResilientDeltas{Layer}(mult > 0) = ResilientDeltas{Layer}(mult > 0) * learningRate_plus; % Sign didn't change
            ResilientDeltas{Layer}(mult < 0) = ResilientDeltas{Layer}(mult < 0) * learningRate_negative; % Sign changed
            ResilientDeltas{Layer} = max(deltas_min, ResilientDeltas{Layer});
            ResilientDeltas{Layer} = min(deltas_max, ResilientDeltas{Layer});

            Old_Delta_Weights_for_Resilient{Layer} = Delta_Weights{Layer};

            Delta_Weights{Layer} = sign(Delta_Weights{Layer}) .* ResilientDeltas{Layer};
        end
    end
    if (enable_learningRate_momentum) %Apply Momentum
        for Layer = 1:nbrOfLayers
            Delta_Weights{Layer} = learningRate*Delta_Weights{Layer} + momentum_alpha*Old_Delta_Weights_for_Momentum{Layer}; 
        end
        Old_Delta_Weights_for_Momentum = Delta_Weights;
    end
    if (~enable_learningRate_momentum && ~enable_resilient_gradient_descent)
        for Layer = 1:nbrOfLayers
            Delta_Weights{Layer} = learningRate * Delta_Weights{Layer};
        end
    end

    %% Backward Pass Weights Update
    for Layer = 1:nbrOfLayers-1
        Weights{Layer} = Weights{Layer} + Delta_Weights{Layer};
    end
    
    % Resetting Delta_Weights to Zeros
    for Layer = 1:length(Delta_Weights)
        Delta_Weights{Layer} = 0 * Delta_Weights{Layer};
    end

    %% Decrease Learning Rate
    if (enable_decrease_learningRate)
        new_learningRate = learningRate - learningRate_decreaseValue;
        learningRate = max(min_learningRate, new_learningRate);
    end
end
%     %% Evaluation
%     for Sample = 1:length(Samples(:,1))
%         outputs = EvaluateNetwork(Samples(Sample,:), NodesActivations, Weights, unipolarBipolarSelector);
%         bound = (1+unipolarBipolarSelector)/2;
%         if (outputs(1) >= bound && outputs(2) < bound) %TODO: Not generic role for any number of output nodes
%             ActualClasses(Sample) = 1;
%         elseif (outputs(1) < bound && outputs(2) >= bound)
%             ActualClasses(Sample) = 0;
%         else
%             if (outputs(1) >= outputs(2))
%                 ActualClasses(Sample) = 1;
%             else
%                 ActualClasses(Sample) = 0;
%             end
%         end
%     end
%     
%     MSE(Epoch) = sum((ActualClasses-TargetClasses).^2)/(length(Samples(:,1)));
%     if (MSE(Epoch) == 0)
%         zeroRMSReached = 1;
%     end
%         
%     %% Visualization
%     if (zeroRMSReached || mod(Epoch,draw_each_nbrOfEpochs)==0)
%         % Draw Decision Boundary
%         unique_TargetClasses = unique(TargetClasses);
%         training_colors = {'y.', 'b.'};
%         separation_colors = {'g.', 'r.'};
%         subplot(2,1,1);
%         cla;
%         hold on;
%         title(['Decision Boundary at Epoch Number ' int2str(Epoch) '. The max number of Epochs is ' int2str(nbrOfEpochs_max) '.']);
% 
%         margin = 0.05; step = 0.05;
%         xlim([min(Samples(:,2))-margin max(Samples(:,2))+margin]);
%         ylim([min(Samples(:,3))-margin max(Samples(:,3))+margin]);
%         for x = min(Samples(:,2))-margin : step : max(Samples(:,2))+margin
%             for y = min(Samples(:,3))-margin : step : max(Samples(:,3))+margin
%                 outputs = EvaluateNetwork([1 x y], NodesActivations, Weights, unipolarBipolarSelector);
%                 bound = (1+unipolarBipolarSelector)/2;
%                 if (outputs(1) >= bound && outputs(2) < bound) %TODO: Not generic role for any number of output nodes
%                     plot(x, y, separation_colors{1}, 'markersize', 18);
%                 elseif (outputs(1) < bound && outputs(2) >= bound)
%                     plot(x, y, separation_colors{2}, 'markersize', 18);
%                 else
%                     if (outputs(1) >= outputs(2))
%                         plot(x, y, separation_colors{1}, 'markersize', 18);
%                     else
%                         plot(x, y, separation_colors{2}, 'markersize', 18);
%                     end
%                 end
%             end
%         end
% 
%         for i = 1:length(unique_TargetClasses)
%             points = Samples(TargetClasses==unique_TargetClasses(i), 2:end);
%             plot(points(:,1), points(:,2), training_colors{i}, 'markersize', 10);
%         end
%         axis equal;
% 
%         % Draw Mean Square Error
%         subplot(2,1,2);
%         MSE(MSE==-1) = [];
%         plot([MSE(1:Epoch)]);
%         ylim([-0.1 0.6]);
%         title('Mean Square Error');
%         xlabel('Epochs');
%         ylabel('MSE');
%         grid on;
% 
%         saveas(gcf, sprintf('Results//fig%i.png', Epoch),'jpg');
%         pause(0.05);
%     end
%     display([int2str(Epoch) ' Epochs done out of ' int2str(nbrOfEpochs_max) ' Epochs. MSE = ' num2str(MSE(Epoch)) ' Learning Rate = ' ...
%         num2str(learningRate) '.']);
%     
%     nbrOfEpochs_done = Epoch;
%     if (zeroRMSReached)
%         saveas(gcf, sprintf('Results//Final Result for %s.png', dataFileName),'jpg');
%         break;
%     end
%     
% end
% display(['Mean Square Error = ' num2str(MSE(nbrOfEpochs_done)) '.']);
